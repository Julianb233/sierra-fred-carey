---
phase: 21-memory-compute-tiers
plan: 01
type: execute
wave: 2
depends_on: []
files_modified:
  - lib/ai/tier-routing.ts
  - lib/agents/base-agent.ts
  - app/api/agents/route.ts
  - lib/constants.ts
  - lib/fred/actors/load-memory.ts
  - app/api/fred/chat/route.ts
autonomous: true

must_haves:
  truths:
    - "tier-routing.ts exports model selection based on user tier (Free->small, Pro->medium, Studio->large)"
    - "base-agent.ts accepts userTier and uses tier-routing to select the appropriate model"
    - "Agent API route passes the user's tier to the agent execution"
    - "MEMORY_CONFIG in constants.ts defines memory depth and retention per tier"
    - "load-memory actor loads tier-appropriate memory depth (Free: 5 messages, Pro: 20, Studio: 50)"
    - "Chat route gates memory features based on tier (Free: session only, Pro+: persistent)"
  artifacts:
    - path: "lib/ai/tier-routing.ts"
      provides: "Tier-based model selection for AI operations"
      exports: ["getModelForTier", "ModelTier", "TIER_MODEL_MAP"]
    - path: "lib/constants.ts"
      provides: "MEMORY_CONFIG with tier-specific depth and retention settings"
      contains: "MEMORY_CONFIG"
    - path: "lib/fred/actors/load-memory.ts"
      provides: "Tier-aware memory loading with configurable depth"
      contains: "MEMORY_CONFIG"
  key_links:
    - from: "lib/ai/tier-routing.ts"
      to: "lib/constants.ts"
      via: "imports tier definitions for model mapping"
      pattern: "import.*from.*constants"
    - from: "lib/agents/base-agent.ts"
      to: "lib/ai/tier-routing.ts"
      via: "calls getModelForTier() to select AI model"
      pattern: "getModelForTier"
    - from: "app/api/agents/route.ts"
      to: "lib/agents/base-agent.ts"
      via: "passes userTier to agent execution"
      pattern: "userTier"
    - from: "lib/fred/actors/load-memory.ts"
      to: "lib/constants.ts"
      via: "reads MEMORY_CONFIG for tier-specific depth"
      pattern: "MEMORY_CONFIG"
    - from: "app/api/fred/chat/route.ts"
      to: "lib/ai/tier-routing.ts"
      via: "selects model based on user tier"
      pattern: "getModelForTier"
---

<objective>
Implement tier-based model routing and memory depth gating so higher-paying tiers get measurably better AI and deeper context.

Purpose: Currently all tiers use the same AI model and memory depth. This plan introduces differentiated compute: Free tier gets a smaller/faster model with session-only memory, Pro gets a medium model with 20-message persistent memory, and Studio gets the best model with 50-message deep memory. This creates tangible value differentiation between tiers and incentivizes upgrades.

Output: 1 new file (tier-routing), 4 modified files (base-agent, agents route, constants, load-memory, chat route). No new npm dependencies. No database changes.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md

Key source files (READ ALL BEFORE MODIFYING):
@lib/agents/base-agent.ts (agent execution -- add tier parameter)
@app/api/agents/route.ts (agent API -- pass tier)
@lib/constants.ts (app constants -- add memory config)
@lib/fred/actors/load-memory.ts (memory loading -- make tier-aware)
@app/api/fred/chat/route.ts (chat route -- add tier-gated memory and model selection)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Model routing and agent wiring</name>
  <files>
    lib/ai/tier-routing.ts
    lib/agents/base-agent.ts
    app/api/agents/route.ts
  </files>
  <action>
**Step 1: Create `lib/ai/tier-routing.ts`**

```typescript
export type ModelTier = "free" | "pro" | "studio";

export interface TierModelConfig {
  chatModel: string;
  agentModel: string;
  structuredModel: string;
  maxTokens: number;
  temperature: number;
}

export const TIER_MODEL_MAP: Record<ModelTier, TierModelConfig> = {
  free: {
    chatModel: "gpt-4o-mini",
    agentModel: "gpt-4o-mini",
    structuredModel: "gpt-4o-mini",
    maxTokens: 1024,
    temperature: 0.7,
  },
  pro: {
    chatModel: "gpt-4o",
    agentModel: "gpt-4o",
    structuredModel: "gpt-4o",
    maxTokens: 2048,
    temperature: 0.7,
  },
  studio: {
    chatModel: "gpt-4o",
    agentModel: "gpt-4o",
    structuredModel: "gpt-4o",
    maxTokens: 4096,
    temperature: 0.7,
  },
};

export function getModelForTier(tier: string, purpose: "chat" | "agent" | "structured" = "chat"): string {
  const normalizedTier = (tier?.toLowerCase() || "free") as ModelTier;
  const config = TIER_MODEL_MAP[normalizedTier] || TIER_MODEL_MAP.free;

  switch (purpose) {
    case "chat": return config.chatModel;
    case "agent": return config.agentModel;
    case "structured": return config.structuredModel;
    default: return config.chatModel;
  }
}

export function getModelConfigForTier(tier: string): TierModelConfig {
  const normalizedTier = (tier?.toLowerCase() || "free") as ModelTier;
  return TIER_MODEL_MAP[normalizedTier] || TIER_MODEL_MAP.free;
}
```

NOTE: Model names should match what the project currently uses. Read the existing model references in the codebase before writing -- if the project uses Anthropic models, adjust accordingly. The tier differentiation concept remains the same.

**Step 2: Update `lib/agents/base-agent.ts`**

Add `userTier?: string` to the agent execution options/config parameter. Import `getModelForTier` from `@/lib/ai/tier-routing`. When creating the AI call, use `getModelForTier(userTier, "agent")` instead of the hardcoded model name.

IMPORTANT: The userTier parameter must be optional with a default of "free" so existing calls without tier still work. Do NOT break the existing agent execution interface.

**Step 3: Update `app/api/agents/route.ts`**

After authenticating the user:
1. Fetch the user's subscription tier using `getUserSubscription()` from the existing tier utilities
2. Pass the tier string to the agent execution call as `userTier`

This ensures agent tasks use the appropriate model for the user's subscription level.
  </action>
  <verify>
1. `test -f lib/ai/tier-routing.ts` -- routing module exists
2. `grep 'getModelForTier' lib/ai/tier-routing.ts lib/agents/base-agent.ts` -- exported and consumed
3. `grep 'userTier' lib/agents/base-agent.ts app/api/agents/route.ts` -- tier passed through
4. `npx tsc --noEmit` passes with no errors
  </verify>
  <done>
Tier routing module at lib/ai/tier-routing.ts maps Free/Pro/Studio to different models and token limits. base-agent.ts accepts optional userTier and routes to appropriate model. Agent API passes user's subscription tier.
  </done>
</task>

<task type="auto">
  <name>Task 2: Memory depth and gating</name>
  <files>
    lib/constants.ts
    lib/fred/actors/load-memory.ts
    app/api/fred/chat/route.ts
  </files>
  <action>
**Step 1: Add MEMORY_CONFIG to `lib/constants.ts`**

Add at the end of the file (do NOT modify existing constants):

```typescript
export const MEMORY_CONFIG = {
  free: {
    maxMessages: 5,         // session context only
    retentionDays: 0,       // no persistent memory
    loadEpisodic: false,    // no episodic memory
    maxEpisodicItems: 0,
  },
  pro: {
    maxMessages: 20,        // recent conversation history
    retentionDays: 30,      // 30-day memory
    loadEpisodic: true,     // episodic memory enabled
    maxEpisodicItems: 10,
  },
  studio: {
    maxMessages: 50,        // deep conversation history
    retentionDays: 90,      // 90-day memory
    loadEpisodic: true,     // episodic memory enabled
    maxEpisodicItems: 25,
  },
} as const;

export type MemoryTier = keyof typeof MEMORY_CONFIG;
```

**Step 2: Update `lib/fred/actors/load-memory.ts`**

Import `MEMORY_CONFIG, MemoryTier` from `@/lib/constants`.

Modify the memory loading function to accept a `tier` parameter (default "free"):
1. Look up the tier's config: `const config = MEMORY_CONFIG[tier as MemoryTier] || MEMORY_CONFIG.free`
2. Limit conversation history to `config.maxMessages` (use `.slice(-config.maxMessages)`)
3. Only load episodic memory if `config.loadEpisodic` is true
4. Limit episodic items to `config.maxEpisodicItems`
5. If `config.retentionDays` is 0, skip persistent memory loading entirely (return empty)

IMPORTANT: Do NOT remove any existing memory loading logic. Only ADD the tier-based limits as filters applied to the existing query results.

**Step 3: Update `app/api/fred/chat/route.ts`**

1. Import `getModelForTier` from `@/lib/ai/tier-routing`
2. After authenticating the user, get their tier from the subscription
3. Pass the tier to the load-memory actor call
4. Use `getModelForTier(tier, "chat")` when making the AI call instead of hardcoded model
5. For Free tier: skip persistent memory saving (session-only). After the response, only save to DB if tier is "pro" or "studio"

IMPORTANT: Do NOT break the existing chat flow. The tier parameter should default gracefully to "free" if unavailable. Memory gating should degrade gracefully -- if memory loading fails, continue with empty context.
  </action>
  <verify>
1. `grep 'MEMORY_CONFIG' lib/constants.ts` -- config exists
2. `grep 'MEMORY_CONFIG' lib/fred/actors/load-memory.ts` -- used in memory loading
3. `grep 'getModelForTier' app/api/fred/chat/route.ts` -- model routing in chat
4. `grep 'tier\|userTier' app/api/fred/chat/route.ts` -- tier-based logic present
5. `npx tsc --noEmit` passes with no errors
  </verify>
  <done>
MEMORY_CONFIG in constants.ts defines per-tier limits (5/20/50 messages, 0/30/90 day retention, episodic toggling). load-memory actor applies tier-based depth limits. Chat route uses tier-routed model selection and gates persistent memory to Pro+ tiers.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Tier routing module:**
   ```bash
   test -f lib/ai/tier-routing.ts && echo "EXISTS"
   grep 'getModelForTier' lib/ai/tier-routing.ts
   ```

2. **Memory config:**
   ```bash
   grep 'MEMORY_CONFIG' lib/constants.ts
   ```

3. **Agent integration:**
   ```bash
   grep 'userTier\|getModelForTier' lib/agents/base-agent.ts app/api/agents/route.ts
   ```

4. **Memory gating:**
   ```bash
   grep 'MEMORY_CONFIG' lib/fred/actors/load-memory.ts
   ```

5. **Chat routing:**
   ```bash
   grep 'getModelForTier' app/api/fred/chat/route.ts
   ```

6. **TypeScript compiles:**
   ```bash
   npx tsc --noEmit
   ```
</verification>

<success_criteria>
- Tier routing module maps Free/Pro/Studio to different models and token limits
- base-agent.ts accepts userTier and routes agent tasks to appropriate model
- Agent API passes user's subscription tier to agent execution
- MEMORY_CONFIG defines per-tier message limits (5/20/50), retention (0/30/90 days), and episodic settings
- load-memory actor applies tier-based depth limits to conversation history and episodic memory
- Chat route uses tier-routed model and gates persistent memory to Pro+ tiers
- Free tier gets session-only memory; Pro gets 30-day; Studio gets 90-day
- All tier parameters default to "free" when unavailable (graceful degradation)
- No existing logic broken -- all changes are additive
- TypeScript compiles with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/21-memory-compute-tiers/21-01-SUMMARY.md`
</output>
