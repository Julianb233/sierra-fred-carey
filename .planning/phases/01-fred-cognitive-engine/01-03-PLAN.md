# Plan 01-03: 7-Factor Scoring Engine

**Status:** COMPLETED
**Phase:** 01 - FRED Cognitive Engine Foundation
**Depends On:** 01-02 (state machine for integration)
**Estimated Complexity:** Medium

---

## Objective

Implement FRED's 7-factor scoring engine for decision quality assessment: strategic alignment, leverage, speed, revenue, time, risk, and relationships.

---

## Context

Every decision FRED makes is scored on 7 factors. This provides:
- Explainable recommendations (why this score?)
- Consistent evaluation (same framework always)
- Calibration tracking (predicted vs actual outcomes)
- User trust (transparent reasoning)

---

## Implementation Steps

### Step 1: Define Scoring Types

**File:** `lib/fred/scoring/types.ts`

```typescript
export interface FactorScores {
  strategicAlignment: FactorScore; // Does this align with stated goals?
  leverage: FactorScore;           // Does this create multiplied impact?
  speed: FactorScore;              // How quickly can this be executed?
  revenue: FactorScore;            // Direct revenue impact
  time: FactorScore;               // Time investment required
  risk: FactorScore;               // Downside exposure
  relationships: FactorScore;      // Impact on key relationships
}

export interface FactorScore {
  value: number;        // 0-1 normalized score
  weight: number;       // Importance weight for this decision type
  confidence: number;   // How confident is this score?
  reasoning: string;    // Why this score?
  evidence: string[];   // Supporting evidence
}

export interface CompositeScore {
  value: number;            // Weighted composite 0-1
  confidence: number;       // Aggregate confidence
  recommendation: 'strong_yes' | 'yes' | 'maybe' | 'no' | 'strong_no';
  factors: FactorScores;
  uncertaintyRange: [number, number]; // 95% confidence interval
}

export interface DecisionType {
  name: string;
  weights: Record<keyof FactorScores, number>; // Custom weights per decision type
}

// Pre-defined decision types with calibrated weights
export const DECISION_TYPES: Record<string, DecisionType> = {
  fundraising: {
    name: 'Fundraising Decision',
    weights: {
      strategicAlignment: 0.2,
      leverage: 0.15,
      speed: 0.1,
      revenue: 0.1,
      time: 0.1,
      risk: 0.2,
      relationships: 0.15
    }
  },
  product: {
    name: 'Product Decision',
    weights: {
      strategicAlignment: 0.25,
      leverage: 0.2,
      speed: 0.15,
      revenue: 0.15,
      time: 0.1,
      risk: 0.1,
      relationships: 0.05
    }
  },
  hiring: {
    name: 'Hiring Decision',
    weights: {
      strategicAlignment: 0.15,
      leverage: 0.2,
      speed: 0.05,
      revenue: 0.1,
      time: 0.15,
      risk: 0.15,
      relationships: 0.2
    }
  },
  general: {
    name: 'General Decision',
    weights: {
      strategicAlignment: 0.15,
      leverage: 0.15,
      speed: 0.15,
      revenue: 0.15,
      time: 0.1,
      risk: 0.15,
      relationships: 0.15
    }
  }
};
```

### Step 2: Create Scoring Engine

**File:** `lib/fred/scoring/engine.ts`

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';
import { openai } from '@ai-sdk/openai';
import type { FactorScores, CompositeScore, DecisionType } from './types';

const factorScoreSchema = z.object({
  value: z.number().min(0).max(1),
  confidence: z.number().min(0).max(1),
  reasoning: z.string(),
  evidence: z.array(z.string())
});

const scoringResponseSchema = z.object({
  strategicAlignment: factorScoreSchema,
  leverage: factorScoreSchema,
  speed: factorScoreSchema,
  revenue: factorScoreSchema,
  time: factorScoreSchema,
  risk: factorScoreSchema,
  relationships: factorScoreSchema
});

export async function scoreDecision(
  decision: string,
  context: DecisionContext,
  decisionType: DecisionType
): Promise<CompositeScore> {
  // Generate factor scores via AI with structured output
  const { object: factors } = await generateObject({
    model: openai('gpt-4o'),
    schema: scoringResponseSchema,
    prompt: buildScoringPrompt(decision, context, decisionType)
  });

  // Calculate weighted composite
  const composite = calculateComposite(factors, decisionType.weights);

  // Determine recommendation threshold
  const recommendation = getRecommendation(composite);

  // Calculate uncertainty range
  const uncertaintyRange = calculateUncertainty(factors);

  return {
    value: composite,
    confidence: aggregateConfidence(factors),
    recommendation,
    factors: addWeights(factors, decisionType.weights),
    uncertaintyRange
  };
}

function calculateComposite(
  factors: FactorScores,
  weights: Record<string, number>
): number {
  let sum = 0;
  let totalWeight = 0;

  for (const [factor, score] of Object.entries(factors)) {
    const weight = weights[factor] || 0;
    sum += score.value * weight * score.confidence;
    totalWeight += weight * score.confidence;
  }

  return totalWeight > 0 ? sum / totalWeight : 0.5;
}

function getRecommendation(score: number): string {
  if (score >= 0.8) return 'strong_yes';
  if (score >= 0.65) return 'yes';
  if (score >= 0.45) return 'maybe';
  if (score >= 0.3) return 'no';
  return 'strong_no';
}

function calculateUncertainty(factors: FactorScores): [number, number] {
  // Use factor confidences to estimate 95% CI
  const confidences = Object.values(factors).map(f => f.confidence);
  const avgConfidence = confidences.reduce((a, b) => a + b, 0) / confidences.length;
  const spread = (1 - avgConfidence) * 0.3; // Higher uncertainty = wider spread

  const composite = calculateComposite(factors, /* equal weights */);
  return [
    Math.max(0, composite - spread),
    Math.min(1, composite + spread)
  ];
}
```

### Step 3: Build Scoring Prompts

**File:** `lib/fred/scoring/prompts.ts`

```typescript
export function buildScoringPrompt(
  decision: string,
  context: DecisionContext,
  decisionType: DecisionType
): string {
  return `
You are FRED, an AI advisor scoring a startup decision using a 7-factor framework.

## Decision to Score
${decision}

## Context
- Startup: ${context.startupName}
- Stage: ${context.stage}
- Industry: ${context.industry}
- Goals: ${context.goals.join(', ')}
- Recent decisions: ${context.recentDecisions.map(d => d.summary).join('; ')}

## Decision Type
${decisionType.name}

## Scoring Framework

Score each factor from 0 to 1, where:
- 0.0-0.2: Very negative impact
- 0.2-0.4: Negative impact
- 0.4-0.6: Neutral/mixed impact
- 0.6-0.8: Positive impact
- 0.8-1.0: Very positive impact

For each factor, provide:
1. A score (0-1)
2. Your confidence in the score (0-1)
3. Brief reasoning (1-2 sentences)
4. Evidence from the context

### Factors

1. **Strategic Alignment**: Does this decision align with the startup's stated goals and strategy?

2. **Leverage**: Does this create multiplied impact? Will it make future efforts easier or more effective?

3. **Speed**: How quickly can this be executed? Consider both implementation time and time-to-impact.

4. **Revenue**: What is the direct or indirect revenue impact? Consider both short and long term.

5. **Time**: What time investment is required from founders/team? Consider opportunity cost.

6. **Risk**: What is the downside exposure? Consider financial, reputational, and operational risks.

7. **Relationships**: How does this affect key relationships (investors, customers, team, partners)?

Be calibrated: express genuine uncertainty when appropriate. A 0.5 score with 0.6 confidence is better than a 0.8 score with 0.3 confidence.
`;
}
```

### Step 4: Calibration Tracking

**File:** `lib/fred/scoring/calibration.ts`

Track predicted scores vs actual outcomes to improve calibration over time.

```typescript
export interface CalibrationRecord {
  id: string;
  decisionId: string;
  predictedScore: number;
  predictedConfidence: number;
  actualOutcome: number | null; // Null until outcome recorded
  outcomeDate: Date | null;
  factors: FactorScores;
}

export async function recordPrediction(
  decisionId: string,
  score: CompositeScore
): Promise<void> { ... }

export async function recordOutcome(
  decisionId: string,
  actualOutcome: number,
  notes: string
): Promise<void> { ... }

export async function getCalibrationMetrics(): Promise<CalibrationMetrics> {
  // Calculate Brier score, calibration curve, etc.
}
```

### Step 5: Integration with State Machine

Update the synthesis actor to use scoring engine.

**File:** `lib/fred/actors/synthesize.ts` (update)

---

## Testing

### Unit Tests
- [x] Test scoring with known inputs (regression tests)
- [x] Test weight application
- [x] Test recommendation thresholds
- [x] Test uncertainty calculation
- [x] Test calibration recording

### Integration Tests
- [x] Test scoring through full decision flow
- [x] Test with various decision types
- [x] Test calibration tracking persistence

---

## Verification

- [x] All 7 factors scored consistently
- [x] Weights correctly applied per decision type
- [x] Recommendations map to correct thresholds
- [x] Uncertainty ranges are reasonable
- [x] Calibration records persist
- [x] Reasoning is human-readable

---

## Notes

- Consider A/B testing different weight configurations
- Calibration metrics should be reviewed weekly initially
- May need to adjust thresholds based on user feedback
- Uncertainty ranges prevent overconfident recommendations

---

*Plan created: 2026-02-05*
