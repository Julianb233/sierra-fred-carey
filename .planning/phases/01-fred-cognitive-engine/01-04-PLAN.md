# Plan 01-04: Vercel AI SDK 6 Integration

**Status:** COMPLETED
**Phase:** 01 - FRED Cognitive Engine Foundation
**Depends On:** 01-02 (state machine actors need AI client)
**Estimated Complexity:** Medium

---

## Objective

Replace the existing custom multi-provider AI client with Vercel AI SDK 6 for unified provider abstraction, structured outputs, and streaming support.

---

## Context

Current codebase has a custom `lib/ai/client.ts` with manual provider fallback. AI SDK 6 provides:
- Unified interface across OpenAI, Anthropic, Google
- Native structured outputs with Zod
- Built-in streaming support
- ToolLoopAgent for complex workflows
- Better TypeScript inference

---

## Implementation Steps

### Step 1: Install AI SDK 6

```bash
pnpm add ai @ai-sdk/openai @ai-sdk/anthropic @ai-sdk/google zod
```

### Step 2: Create Provider Configuration

**File:** `lib/ai/providers.ts`

```typescript
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google } from '@ai-sdk/google';

// Provider configurations with fallback order
export const providers = {
  primary: openai('gpt-4o'),
  fallback1: anthropic('claude-3-5-sonnet-20241022'),
  fallback2: google('gemini-1.5-pro'),

  // Specialized models
  fast: openai('gpt-4o-mini'),
  embedding: openai.embedding('text-embedding-3-small'),
};

export type ProviderKey = keyof typeof providers;
```

### Step 3: Create FRED AI Client

**File:** `lib/ai/fred-client.ts`

```typescript
import { generateText, generateObject, streamText, embed } from 'ai';
import { z } from 'zod';
import { providers } from './providers';

export interface GenerateOptions {
  model?: ProviderKey;
  maxTokens?: number;
  temperature?: number;
}

export async function generate(
  prompt: string,
  options: GenerateOptions = {}
): Promise<string> {
  const model = providers[options.model || 'primary'];

  const { text } = await generateText({
    model,
    prompt,
    maxTokens: options.maxTokens || 4096,
    temperature: options.temperature || 0.7,
  });

  return text;
}

export async function generateStructured<T extends z.ZodType>(
  prompt: string,
  schema: T,
  options: GenerateOptions = {}
): Promise<z.infer<T>> {
  const model = providers[options.model || 'primary'];

  const { object } = await generateObject({
    model,
    prompt,
    schema,
    maxTokens: options.maxTokens || 4096,
  });

  return object;
}

export async function* streamGenerate(
  prompt: string,
  options: GenerateOptions = {}
): AsyncGenerator<string> {
  const model = providers[options.model || 'primary'];

  const { textStream } = streamText({
    model,
    prompt,
    maxTokens: options.maxTokens || 4096,
    temperature: options.temperature || 0.7,
  });

  for await (const chunk of textStream) {
    yield chunk;
  }
}

export async function generateEmbedding(text: string): Promise<number[]> {
  const { embedding } = await embed({
    model: providers.embedding,
    value: text,
  });

  return embedding;
}
```

### Step 4: Create Structured Output Schemas

**File:** `lib/ai/schemas/index.ts`

```typescript
import { z } from 'zod';

// Input validation schema
export const validatedInputSchema = z.object({
  intent: z.enum(['question', 'decision_request', 'information', 'feedback']),
  entities: z.array(z.object({
    type: z.string(),
    value: z.string(),
    confidence: z.number().min(0).max(1),
  })),
  confidence: z.number().min(0).max(1),
  clarificationNeeded: z.array(z.string()),
});

// Mental model result schema
export const mentalModelSchema = z.object({
  model: z.string(),
  analysis: z.record(z.unknown()),
  relevance: z.number().min(0).max(1),
  keyInsights: z.array(z.string()),
});

// Synthesis result schema
export const synthesisSchema = z.object({
  recommendation: z.string(),
  confidence: z.number().min(0).max(1),
  reasoning: z.string(),
  alternatives: z.array(z.object({
    option: z.string(),
    pros: z.array(z.string()),
    cons: z.array(z.string()),
  })),
});

// Decision result schema
export const decisionSchema = z.object({
  action: z.enum(['auto_execute', 'recommend', 'escalate']),
  content: z.string(),
  confidence: z.number().min(0).max(1),
  requiresHumanApproval: z.boolean(),
  reasoning: z.string(),
});

export type ValidatedInput = z.infer<typeof validatedInputSchema>;
export type MentalModelResult = z.infer<typeof mentalModelSchema>;
export type SynthesisResult = z.infer<typeof synthesisSchema>;
export type DecisionResult = z.infer<typeof decisionSchema>;
```

### Step 5: Update Existing Chat Endpoint

**File:** `app/api/chat/route.ts` (update)

Migrate from custom client to new fred-client while maintaining backward compatibility.

### Step 6: Add Request/Response Logging

**File:** `lib/ai/logging.ts`

```typescript
import { sql } from '../db/supabase-sql';

export async function logAIRequest(params: {
  userId: string;
  model: string;
  prompt: string;
  requestId: string;
}): Promise<void> {
  await sql`
    insert into ai_requests (id, user_id, model, prompt, created_at)
    values (${params.requestId}, ${params.userId}, ${params.model}, ${params.prompt}, now())
  `;
}

export async function logAIResponse(params: {
  requestId: string;
  response: string;
  tokens: { prompt: number; completion: number };
  latencyMs: number;
}): Promise<void> {
  await sql`
    insert into ai_responses (request_id, response, prompt_tokens, completion_tokens, latency_ms, created_at)
    values (${params.requestId}, ${params.response}, ${params.tokens.prompt}, ${params.tokens.completion}, ${params.latencyMs}, now())
  `;
}
```

---

## Migration Notes

The existing `lib/ai/client.ts` should be deprecated but kept for backward compatibility during migration. New code should use `lib/ai/fred-client.ts`.

Key differences:
- Old: `aiClient.chat(messages)` → New: `generate(prompt)` or `generateStructured(prompt, schema)`
- Old: Manual fallback chain → New: AI SDK handles retries
- Old: Raw JSON responses → New: Zod-validated typed responses

---

## Testing

### Unit Tests
- [x] Test generate with each provider
- [x] Test generateStructured with Zod schemas
- [x] Test streamGenerate chunking
- [x] Test embedding generation
- [x] Test fallback behavior on provider errors

### Integration Tests
- [x] Test chat endpoint with new client
- [x] Test request/response logging
- [ ] Test with real API keys (requires live API keys)

---

## Verification

- [x] AI SDK 6 packages installed
- [x] Provider configuration works
- [x] Structured outputs validate correctly
- [x] Streaming works end-to-end
- [x] Embeddings generate with correct dimensions
- [x] Logging captures all requests/responses
- [x] Existing chat endpoint still functions

---

*Plan created: 2026-02-05*
