---
phase: 63-fred-intelligence-upgrade
plan: 03
type: execute
wave: 2
depends_on: ["63-01"]
files_modified:
  - lib/ai/context-manager.ts
  - app/api/fred/chat/route.ts
autonomous: true

must_haves:
  truths:
    - "Long conversations (50+ messages) do not degrade FRED's response quality"
    - "Older messages are summarized rather than dropped when context window is full"
    - "Context trimming is wired into the chat route before sending to the LLM"
  artifacts:
    - path: "lib/ai/context-manager.ts"
      provides: "Conversation summarization function for long threads"
      contains: "summarizeOlderMessages"
    - path: "app/api/fred/chat/route.ts"
      provides: "Context trimming wired into the chat pipeline"
      contains: "trimMessages\|summarizeOlderMessages"
  key_links:
    - from: "app/api/fred/chat/route.ts"
      to: "lib/ai/context-manager.ts"
      via: "import and call trimMessages/summarizeOlderMessages"
      pattern: "context-manager"
    - from: "lib/ai/context-manager.ts"
      to: "lib/ai/fred-client.ts"
      via: "generate() call for summarization"
      pattern: "generate"
---

<objective>
Wire the existing context-manager.ts into the chat route and add conversation summarization for long threads.

Purpose: context-manager.ts exists with token estimation and message trimming but is NOT imported or used anywhere in the chat route. Long conversations (50+ messages) will exceed the context window. This plan adds a `summarizeOlderMessages()` function and wires the context manager into the chat pipeline so older messages are summarized instead of silently lost.

Output: Long conversations handled gracefully with summarization, preventing quality degradation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/63-fred-intelligence-upgrade/63-RESEARCH.md
@.planning/phases/63-fred-intelligence-upgrade/63-01-SUMMARY.md

@lib/ai/context-manager.ts
@app/api/fred/chat/route.ts
@lib/ai/fred-client.ts
@lib/fred/service.ts
@lib/fred/actors/decide.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add conversation summarization to context-manager.ts</name>
  <files>lib/ai/context-manager.ts</files>
  <action>
    1. Add a new exported async function `summarizeOlderMessages()`:
       ```
       export async function summarizeOlderMessages(
         messages: Message[],
         keepRecent: number = 10
       ): Promise<Message[]>
       ```
       - If messages.length <= keepRecent, return messages unchanged.
       - Split into `older = messages.slice(0, -keepRecent)` and `recent = messages.slice(-keepRecent)`.
       - Filter older to only include user and assistant messages (skip system).
       - If no older messages after filtering, return messages unchanged.
       - Import `generate` from `@/lib/ai/fred-client`.
       - Call `generate()` with a summarization prompt:
         - System: "Create a concise summary of a conversation between a founder and their mentor FRED. Preserve: key decisions made, important facts shared, action items discussed, the founder's current situation and challenges. Keep it under 500 words."
         - User message: The formatted older messages as `"[role]: content\n"` joined.
         - maxOutputTokens: 512
         - temperature: 0.3 (factual, not creative)
       - Return: `[{ role: "system", content: "Previous conversation summary:\n" + summary.text }, ...recent]`
       - Wrap the entire generate call in try/catch. On failure, log a warning and fall back to `trimMessages(messages)` (existing function) to at least keep recent messages.

    2. Add a new exported function `shouldSummarize()`:
       ```
       export function shouldSummarize(
         messages: Message[],
         systemPromptTokens: number,
         model: ProviderKey = "primary"
       ): boolean
       ```
       - Returns true if: `messages.length > 20` AND `estimateMessagesTokens(messages).tokens + systemPromptTokens > MODEL_CONTEXT_LIMITS[model] * 0.6`
       - The 0.6 threshold triggers summarization before hitting the hard limit, leaving room for the response and some buffer.
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `grep -n "summarizeOlderMessages\|shouldSummarize" lib/ai/context-manager.ts` shows both functions
  </verify>
  <done>
    context-manager.ts has summarizeOlderMessages (LLM-based) and shouldSummarize (heuristic check). Summarization preserves key decisions/facts from older messages while keeping the 10 most recent messages intact.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire context manager into the chat route</name>
  <files>app/api/fred/chat/route.ts</files>
  <action>
    1. The chat route currently assembles `fullContext` (the system prompt blocks) but does NOT manage conversation history length. The actual conversation history is managed differently depending on whether the machine or direct LLM call is used.

    2. Read `lib/fred/service.ts` and `lib/fred/actors/decide.ts` to trace where conversation history is assembled and sent to the LLM. The `generate()` call in `decide.ts` (line ~289) sends `input.originalMessage` with a system prompt — this is a single-turn call, not multi-turn. The conversation history context comes from episodic memory loaded by loadMemoryActor.

    3. The key integration point is: the episodic memory loaded in Plan 01 now contains relevant past episodes. These episodes are formatted into the system prompt via `founderContext` / `fullContext`. The context manager needs to ensure `fullContext` (which includes founderContext + all the mode/framework/gate blocks) does not exceed the token budget.

    4. Add context trimming to the `fullContext` assembly (around line 482-492 in route.ts):
       - Import `estimateTokens` from `@/lib/ai/context-manager`
       - After assembling `fullContext`, check its token count: `const contextTokens = estimateTokens(fullContext)`
       - If `contextTokens > 100_000` (leaving ~28K for response + conversation), truncate the least critical blocks. Priority order to keep: founderContext, frameworkBlock, stepGuidanceBlock, rlStatusBlock, rlGateBlock, modeTransitionBlock, irsBlock, deckProtocolBlock, deckReviewReadyBlock.
       - If still too large, truncate founderContext to first 80K tokens worth of chars.
       - Log a warning when truncation happens: `console.warn("[FRED Chat] Context truncated: ${contextTokens} tokens -> ${truncatedTokens} tokens")`

    5. Additionally, for the episodic memory results that Plan 01 enhanced:
       - The memory episodes are injected into the system prompt by `buildFounderContextWithFacts()`. Check `lib/fred/context-builder.ts` to verify. If the context builder formats episodes into the founderContext string, the token estimation on fullContext will catch oversized context.
       - If episodes are NOT in fullContext but instead passed separately to the machine, add trimming at that point instead.

    IMPORTANT: Read the actual code flow carefully before making changes. The goal is to ensure the TOTAL context sent to the LLM (system prompt + all injected blocks + conversation) stays within 128K tokens with 4K reserved for response. The existing `trimMessages()` function in context-manager.ts can be used for message-level trimming if needed.
  </action>
  <verify>
    - `grep -n "estimateTokens\|context-manager" app/api/fred/chat/route.ts` shows context manager imported and used
    - `npx tsc --noEmit` passes
    - `npm test -- --passWithNoTests` passes
  </verify>
  <done>
    Context manager wired into chat route. System prompt context is token-estimated and truncated if it exceeds budget. Long conversation context handled through summarization or trimming.
  </done>
</task>

</tasks>

<verification>
- TypeScript compiles without errors: `npx tsc --noEmit`
- Existing tests pass: `npm test -- --passWithNoTests`
- context-manager.ts has summarizeOlderMessages and shouldSummarize
- Chat route imports and uses context manager for token estimation
- Large system prompts are truncated gracefully with logging
</verification>

<success_criteria>
1. summarizeOlderMessages function exists and uses LLM to summarize older messages
2. shouldSummarize function provides a heuristic check for when summarization is needed
3. Chat route estimates fullContext tokens and truncates when over budget
4. No regressions — short conversations work exactly as before
</success_criteria>

<output>
After completion, create `.planning/phases/63-fred-intelligence-upgrade/63-03-SUMMARY.md`
</output>
