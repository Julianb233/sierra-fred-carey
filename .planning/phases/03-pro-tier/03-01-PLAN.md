# Plan 03-01: PDF Document Pipeline

**Status:** COMPLETED
**Phase:** 03 - Pro Tier Features
**Depends On:** 01-01 (Database schema), 02-04 (Tier gating)
**Estimated Complexity:** High

---

## Objective

Build a robust PDF processing pipeline that:
1. Accepts PDF uploads from Pro/Studio users
2. Extracts text content intelligently
3. Chunks documents semantically
4. Generates vector embeddings for RAG
5. Stores documents securely with user scoping

---

## Current State Analysis

**Existing:**
- Supabase database with pgvector extension
- Tier gating middleware
- File upload patterns in Next.js

**Missing:**
- PDF processing library integration
- Document chunking logic
- Embedding generation
- Document storage schema
- Upload UI component

---

## Implementation Steps

### Step 1: Add Database Schema for Documents

**File:** `lib/db/migrations/009_documents.sql`

```sql
-- Documents table
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  name TEXT NOT NULL,
  type TEXT NOT NULL, -- 'pitch_deck', 'financial', 'strategy', 'other'
  file_url TEXT NOT NULL,
  file_size INTEGER NOT NULL,
  page_count INTEGER,
  status TEXT NOT NULL DEFAULT 'processing', -- 'processing', 'ready', 'failed'
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Document chunks for RAG
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
  chunk_index INTEGER NOT NULL,
  content TEXT NOT NULL,
  embedding vector(1536),
  metadata JSONB DEFAULT '{}', -- page_number, section, etc.
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_documents_user ON documents(user_id);
CREATE INDEX idx_document_chunks_document ON document_chunks(document_id);
CREATE INDEX idx_document_chunks_embedding ON document_chunks
  USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

### Step 2: Create PDF Processor

**File:** `lib/documents/pdf-processor.ts`

```typescript
// Use pdf-parse for text extraction
// Handle multi-page documents
// Extract metadata (page count, title, etc.)
// Error handling for corrupted/encrypted PDFs
```

Key functions:
- `extractText(buffer: Buffer): Promise<ExtractedDocument>`
- `getMetadata(buffer: Buffer): Promise<DocumentMetadata>`

### Step 3: Create Document Chunker

**File:** `lib/documents/chunker.ts`

```typescript
// Semantic chunking strategies:
// 1. By page (for pitch decks)
// 2. By section/heading (for long documents)
// 3. By paragraph with overlap (for dense text)

// Target chunk size: 500-1000 tokens
// Include metadata about source location
```

Key functions:
- `chunkDocument(text: string, options: ChunkOptions): Chunk[]`
- `chunkByPage(pages: Page[]): Chunk[]`
- `chunkBySemantic(text: string, maxTokens: number): Chunk[]`

### Step 4: Create Embedding Generator

**File:** `lib/documents/embeddings.ts`

```typescript
// Use OpenAI text-embedding-3-small (1536 dimensions)
// Batch embedding requests for efficiency
// Handle rate limits gracefully
```

Key functions:
- `generateEmbedding(text: string): Promise<number[]>`
- `generateEmbeddings(texts: string[]): Promise<number[][]>`

### Step 5: Create Document Storage Operations

**File:** `lib/db/documents.ts`

```typescript
// CRUD operations for documents
// User-scoped queries
// Search by embedding similarity
```

Key functions:
- `createDocument(userId, data): Promise<Document>`
- `getDocuments(userId, filters): Promise<Document[]>`
- `getDocumentById(userId, id): Promise<Document | null>`
- `deleteDocument(userId, id): Promise<void>`
- `storeChunks(documentId, chunks): Promise<void>`
- `searchSimilarChunks(embedding, limit): Promise<Chunk[]>`

### Step 6: Create Upload API Endpoint

**File:** `app/api/documents/upload/route.ts`

```typescript
// POST /api/documents/upload
// - Require Pro tier
// - Accept multipart form data
// - Validate file type (PDF only) and size (max 10MB)
// - Upload to Supabase Storage
// - Create document record
// - Queue processing job
```

### Step 7: Create Document Processing Job

**File:** `lib/documents/process-document.ts`

```typescript
// Async processing pipeline:
// 1. Download from storage
// 2. Extract text
// 3. Chunk document
// 4. Generate embeddings
// 5. Store chunks
// 6. Update document status
```

### Step 8: Create Upload UI Component

**File:** `components/documents/upload-zone.tsx`

Features:
- Drag and drop support
- File type validation
- Size limit display
- Upload progress
- Error handling
- Success state with document preview

### Step 9: Create Documents List Component

**File:** `components/documents/document-list.tsx`

Features:
- Grid/list view toggle
- Document type icons
- Status indicators
- Delete confirmation
- Link to analysis features

---

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/documents/upload` | POST | Upload new document |
| `/api/documents` | GET | List user's documents |
| `/api/documents/[id]` | GET | Get document details |
| `/api/documents/[id]` | DELETE | Delete document |
| `/api/documents/[id]/search` | POST | Search within document |

---

## Testing

- [ ] PDF extraction works for various document types
- [ ] Chunking produces reasonable segments
- [ ] Embeddings generated correctly
- [ ] Documents stored with user isolation
- [ ] Upload size limits enforced
- [ ] Processing errors handled gracefully
- [ ] Pro tier requirement enforced

---

## Verification

- [ ] Upload a pitch deck PDF
- [ ] Verify text extracted correctly
- [ ] Check chunks in database
- [ ] Verify embeddings have correct dimensions
- [ ] Test similarity search
- [ ] Verify non-Pro users blocked

---

## Implementation Summary

**Files Created:**
- `lib/db/migrations/024_uploaded_documents.sql` - Database schema for uploaded docs and chunks
- `lib/documents/types.ts` - TypeScript types for documents and chunks
- `lib/documents/pdf-processor.ts` - PDF text extraction using pdf-parse
- `lib/documents/chunker.ts` - Semantic, page, and fixed-size chunking
- `lib/documents/embeddings.ts` - OpenAI embedding generation
- `lib/documents/process-document.ts` - Full processing pipeline
- `lib/documents/index.ts` - Module exports
- `lib/db/documents.ts` - Database CRUD operations
- `app/api/documents/upload/route.ts` - Upload endpoint (Pro tier)
- `app/api/documents/uploaded/route.ts` - List uploaded documents
- `app/api/documents/uploaded/[id]/route.ts` - Document details/delete
- `app/api/documents/[id]/search/route.ts` - Vector similarity search
- `components/documents/upload-zone.tsx` - Upload UI (already existed)
- `components/documents/document-list.tsx` - Document list UI

**Features:**
- Drag-and-drop PDF upload
- 10MB file size limit
- PDF text extraction with metadata
- Three chunking strategies (page, semantic, fixed)
- OpenAI text-embedding-3-small embeddings (1536 dimensions)
- pgvector similarity search
- Pro tier gating on all endpoints
- Async processing pipeline

---

*Plan completed: 2026-02-05*
